{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48cd6130-7690-4d82-8b2c-84f3e3e45d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "Working Directory: C:\\Users\\Ashutosh\\Documents\\Projects\\beyond-smote-evaluation\\notebooks\n",
      "Random Seed: 42\n",
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Available: True\n",
      "CUDA Device: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Importing Required Libraries\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Adding src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Importing custom utilities\n",
    "from config import *\n",
    "from data_utils import retrieve_processed_datasets, load_resampled_data\n",
    "from pytorch_models import (\n",
    "    BatchNormClassifier,\n",
    "    WeightedLossClassifier,\n",
    "    calculate_pos_weight,\n",
    "    prepare_data_loaders,\n",
    "    train_pytorch_model,\n",
    "    evaluate_pytorch_model,\n",
    "    save_pytorch_model,\n",
    "    load_pytorch_model\n",
    ")\n",
    "from evaluation_utils import (\n",
    "    calculate_all_metrics,\n",
    "    create_confusion_matrix,\n",
    "    plot_roc_curve,\n",
    "    plot_precision_recall_curve\n",
    ")\n",
    "\n",
    "# Setting visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Setting PyTorch random seed for reproducibility\n",
    "torch.manual_seed(SEED_VALUE)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED_VALUE)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"Working Directory: {Path.cwd()}\")\n",
    "print(f\"Random Seed: {SEED_VALUE}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7060edc6-2fbf-4505-800a-4ef3ff8a6cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING DATA FOR PYTORCH\n",
      "======================================================================\n",
      "Loading processed data from C:\\Users\\Ashutosh\\Documents\\Projects\\beyond-smote-evaluation\\data\\processed...\n",
      "Datasets loaded successfully\n",
      "Training shape: (800000, 28)\n",
      "Testing shape: (200000, 28)\n",
      "\n",
      "Data loaded successfully!\n",
      "Training set: (800000, 28)\n",
      "Test set: (200000, 28)\n",
      "\n",
      "Train-Validation Split:\n",
      "  Training: 640,000 samples\n",
      "  Validation: 160,000 samples\n",
      "  Test: 200,000 samples\n",
      "\n",
      "Class Distribution:\n",
      "  Training - Signal: 338,990 (52.97%)\n",
      "  Validation - Signal: 84,748 (52.97%)\n",
      "  Test - Signal: 105,935 (52.97%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Loading Data and Create Train-Validation Split\n",
    "# Loading processed data and creating validation split for PyTorch\n",
    "# PyTorch needs separate validation set for early stopping\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING DATA FOR PYTORCH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Loading original processed datasets\n",
    "X_train_full, X_test, y_train_full, y_test = retrieve_processed_datasets(\n",
    "    file_prefix='higgs'\n",
    ")\n",
    "\n",
    "print(\"\\nData loaded successfully!\")\n",
    "print(f\"Training set: {X_train_full.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Creating train-validation split from training data\n",
    "# Using 80-20 split for train-val\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED_VALUE,\n",
    "    stratify=y_train_full\n",
    ")\n",
    "\n",
    "print(\"\\nTrain-Validation Split:\")\n",
    "print(f\"  Training: {X_train.shape[0]:,} samples\")\n",
    "print(f\"  Validation: {X_val.shape[0]:,} samples\")\n",
    "print(f\"  Test: {X_test.shape[0]:,} samples\")\n",
    "\n",
    "# Displaying class distribution\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(f\"  Training - Signal: {y_train.sum():,} ({(y_train.sum()/len(y_train))*100:.2f}%)\")\n",
    "print(f\"  Validation - Signal: {y_val.sum():,} ({(y_val.sum()/len(y_val))*100:.2f}%)\")\n",
    "print(f\"  Test - Signal: {y_test.sum():,} ({(y_test.sum()/len(y_test))*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f05620e6-0dc6-4a69-9fe6-a0f83a37c5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CALCULATING CLASS WEIGHTS\n",
      "======================================================================\n",
      "\n",
      "Class Weight Calculation:\n",
      "  Negative Class Count: 301,010\n",
      "  Positive Class Count: 338,990\n",
      "  Positive Weight: 0.8880\n",
      "\n",
      "Interpretation:\n",
      "  Loss for minority class samples will be weighted 0.89x higher\n",
      "  This compensates for class imbalance without data resampling\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Calculating Class Weights for Weighted Loss\n",
    "# Computing positive class weight for imbalanced learning\n",
    "# Method: weight = n_negative / n_positive\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CALCULATING CLASS WEIGHTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculating positive weight\n",
    "pos_weight = calculate_pos_weight(y_train.values)\n",
    "\n",
    "print(f\"\\nClass Weight Calculation:\")\n",
    "print(f\"  Negative Class Count: {(y_train == 0).sum():,}\")\n",
    "print(f\"  Positive Class Count: {(y_train == 1).sum():,}\")\n",
    "print(f\"  Positive Weight: {pos_weight.item():.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"  Loss for minority class samples will be weighted {pos_weight.item():.2f}x higher\")\n",
    "print(f\"  This compensates for class imbalance without data resampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86275d7a-2c90-401a-b533-864eaed2380a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INITIALIZING PYTORCH MODELS\n",
      "======================================================================\n",
      "\n",
      "Model 1: BatchNormClassifier\n",
      "  Architecture: 28 -> 256 -> 128 -> 64 -> 32 -> 1\n",
      "  Total Parameters: 51,649\n",
      "  Trainable Parameters: 51,649\n",
      "\n",
      "Model 2: WeightedLossClassifier\n",
      "  Architecture: 28 -> 256 -> 128 -> 64 -> 32 -> 1\n",
      "  Total Parameters: 51,649\n",
      "\n",
      "Both models use:\n",
      "  - Batch Normalization after each layer\n",
      "  - ReLU activation\n",
      "  - Dropout: [0.3, 0.3, 0.2]\n",
      "  - Sigmoid output\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Initializing PyTorch Models\n",
    "# Creating instances of both PyTorch model architectures\n",
    "# Models: BatchNormClassifier, WeightedLossClassifier\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INITIALIZING PYTORCH MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Getting input dimension\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Initializing BatchNormClassifier (for use with resampled data)\n",
    "model_batchnorm = BatchNormClassifier(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dims=DL_CONFIG['architecture'],\n",
    "    dropout_rates=DL_CONFIG['dropout_vals']\n",
    ")\n",
    "\n",
    "# Initializing WeightedLossClassifier (for use with weighted loss)\n",
    "model_weighted = WeightedLossClassifier(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dims=DL_CONFIG['architecture'],\n",
    "    dropout_rates=DL_CONFIG['dropout_vals']\n",
    ")\n",
    "\n",
    "print(\"\\nModel 1: BatchNormClassifier\")\n",
    "print(f\"  Architecture: {input_dim} -> {' -> '.join(map(str, DL_CONFIG['architecture']))} -> 1\")\n",
    "print(f\"  Total Parameters: {sum(p.numel() for p in model_batchnorm.parameters()):,}\")\n",
    "print(f\"  Trainable Parameters: {sum(p.numel() for p in model_batchnorm.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "print(\"\\nModel 2: WeightedLossClassifier\")\n",
    "print(f\"  Architecture: {input_dim} -> {' -> '.join(map(str, DL_CONFIG['architecture']))} -> 1\")\n",
    "print(f\"  Total Parameters: {sum(p.numel() for p in model_weighted.parameters()):,}\")\n",
    "\n",
    "print(\"\\nBoth models use:\")\n",
    "print(f\"  - Batch Normalization after each layer\")\n",
    "print(f\"  - ReLU activation\")\n",
    "print(f\"  - Dropout: {DL_CONFIG['dropout_vals']}\")\n",
    "print(f\"  - Sigmoid output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d64144-2757-4d39-b18f-7a3469e1ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENT 1: BATCHNORM + BASELINE (No Resampling)\n",
      "======================================================================\n",
      "\n",
      "Data loaders created:\n",
      "  Training batches: 1250\n",
      "  Validation batches: 313\n",
      "\n",
      "Training BatchNormClassifier on baseline data...\n",
      "Epochs: 50, Learning Rate: 0.001\n",
      "Epoch [5/50] - Train Loss: 0.5334, Train Acc: 0.7279, Val Loss: 0.5188, Val Acc: 0.7376\n",
      "Epoch [10/50] - Train Loss: 0.5219, Train Acc: 0.7359, Val Loss: 0.5069, Val Acc: 0.7455\n",
      "Epoch [15/50] - Train Loss: 0.5178, Train Acc: 0.7392, Val Loss: 0.5043, Val Acc: 0.7483\n",
      "Epoch [20/50] - Train Loss: 0.5143, Train Acc: 0.7411, Val Loss: 0.5011, Val Acc: 0.7498\n",
      "Epoch [25/50] - Train Loss: 0.5123, Train Acc: 0.7431, Val Loss: 0.4988, Val Acc: 0.7516\n",
      "Epoch [30/50] - Train Loss: 0.5107, Train Acc: 0.7434, Val Loss: 0.4985, Val Acc: 0.7530\n",
      "Epoch [35/50] - Train Loss: 0.5095, Train Acc: 0.7443, Val Loss: 0.4970, Val Acc: 0.7526\n",
      "Epoch [40/50] - Train Loss: 0.5087, Train Acc: 0.7450, Val Loss: 0.4957, Val Acc: 0.7532\n",
      "Epoch [45/50] - Train Loss: 0.5073, Train Acc: 0.7459, Val Loss: 0.4954, Val Acc: 0.7537\n",
      "Epoch [50/50] - Train Loss: 0.5065, Train Acc: 0.7458, Val Loss: 0.4950, Val Acc: 0.7534\n",
      "Training completed in 578.28 seconds\n",
      "Best validation loss: 0.4944\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Accuracy: 0.7549\n",
      "Prediction time: 1.7129 seconds\n",
      "\n",
      "Baseline PyTorch Performance:\n",
      "  Accuracy: 0.7549\n",
      "  Precision: 0.7702\n",
      "  Recall: 0.7655\n",
      "  F1-Score: 0.7679\n",
      "  AUC-ROC: 0.8389\n",
      "  AUC-PR: 0.8521\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Experiment 1 - BatchNorm with Baseline (No Resampling)\n",
    "# Establishing PyTorch baseline on original imbalanced data\n",
    "# Lower performance is expected due to imbalance\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 1: BATCHNORM + BASELINE (No Resampling)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Preparing data loaders for baseline\n",
    "train_loader_baseline, val_loader_baseline = prepare_data_loaders(\n",
    "    X_train=X_train.values,\n",
    "    y_train=y_train.values,\n",
    "    X_val=X_val.values,\n",
    "    y_val=y_val.values,\n",
    "    batch_size=DL_CONFIG['batch_count']\n",
    ")\n",
    "\n",
    "print(f\"\\nData loaders created:\")\n",
    "print(f\"  Training batches: {len(train_loader_baseline)}\")\n",
    "print(f\"  Validation batches: {len(val_loader_baseline)}\")\n",
    "\n",
    "# Training model\n",
    "print(f\"\\nTraining BatchNormClassifier on baseline data...\")\n",
    "print(f\"Epochs: {DL_CONFIG['epoch_count']}, Learning Rate: {DL_CONFIG['learn_rate']}\")\n",
    "\n",
    "model_baseline = BatchNormClassifier(input_dim=input_dim)\n",
    "\n",
    "trained_baseline, history_baseline = train_pytorch_model(\n",
    "    model=model_baseline,\n",
    "    train_loader=train_loader_baseline,\n",
    "    val_loader=val_loader_baseline,\n",
    "    epochs=DL_CONFIG['epoch_count'],\n",
    "    learning_rate=DL_CONFIG['learn_rate'],\n",
    "    pos_weight=None,  # No weighting for baseline\n",
    "    patience=DL_CONFIG['patience'],\n",
    "    device=DL_CONFIG['compute_device'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Evaluating on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "baseline_results = evaluate_pytorch_model(\n",
    "    model=trained_baseline,\n",
    "    X_test=X_test.values,\n",
    "    y_test=y_test.values,\n",
    "    batch_size=DL_CONFIG['batch_count'],\n",
    "    device=DL_CONFIG['compute_device'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Computing comprehensive metrics\n",
    "baseline_metrics = calculate_all_metrics(\n",
    "    y_true=y_test.values,\n",
    "    y_pred=baseline_results['y_pred'],\n",
    "    y_proba=baseline_results['y_proba']\n",
    ")\n",
    "\n",
    "print(\"\\nBaseline PyTorch Performance:\")\n",
    "print(f\"  Accuracy: {baseline_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {baseline_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall: {baseline_metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score: {baseline_metrics['f1_score']:.4f}\")\n",
    "print(f\"  AUC-ROC: {baseline_metrics['auc_roc']:.4f}\")\n",
    "print(f\"  AUC-PR: {baseline_metrics['auc_pr']:.4f}\")\n",
    "\n",
    "# Storing results\n",
    "pytorch_results = {\n",
    "    'baseline_batchnorm': {\n",
    "        'model': trained_baseline,\n",
    "        'history': history_baseline,\n",
    "        'metrics': baseline_metrics,\n",
    "        'predictions': baseline_results\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caedde63-bae5-4046-b9a5-f2e9e8d1de5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENT 2: BATCHNORM + SMOTE\n",
      "======================================================================\n",
      "Loading resampled data: smote\n",
      "  Loaded 847,476 samples\n",
      "  Features shape: (847476, 28)\n",
      "\n",
      "SMOTE dataset loaded: 847,476 samples\n",
      "\n",
      "Training BatchNormClassifier on SMOTE data...\n",
      "Epoch [5/50] - Train Loss: 0.5322, Train Acc: 0.7295, Val Loss: 0.5151, Val Acc: 0.7418\n",
      "Epoch [10/50] - Train Loss: 0.5205, Train Acc: 0.7373, Val Loss: 0.5045, Val Acc: 0.7486\n",
      "Epoch [15/50] - Train Loss: 0.5159, Train Acc: 0.7401, Val Loss: 0.5000, Val Acc: 0.7513\n",
      "Epoch [20/50] - Train Loss: 0.5129, Train Acc: 0.7421, Val Loss: 0.4985, Val Acc: 0.7526\n",
      "Epoch [25/50] - Train Loss: 0.5111, Train Acc: 0.7437, Val Loss: 0.4973, Val Acc: 0.7537\n",
      "Epoch [30/50] - Train Loss: 0.5091, Train Acc: 0.7453, Val Loss: 0.4953, Val Acc: 0.7548\n",
      "Epoch [35/50] - Train Loss: 0.5076, Train Acc: 0.7458, Val Loss: 0.4940, Val Acc: 0.7557\n",
      "Epoch [40/50] - Train Loss: 0.5065, Train Acc: 0.7469, Val Loss: 0.4929, Val Acc: 0.7573\n",
      "Epoch [45/50] - Train Loss: 0.5058, Train Acc: 0.7464, Val Loss: 0.4922, Val Acc: 0.7565\n",
      "Epoch [50/50] - Train Loss: 0.5054, Train Acc: 0.7472, Val Loss: 0.4921, Val Acc: 0.7581\n",
      "Training completed in 624.16 seconds\n",
      "Best validation loss: 0.4917\n",
      "Test Accuracy: 0.7549\n",
      "Prediction time: 1.9216 seconds\n",
      "\n",
      "SMOTE PyTorch Performance:\n",
      "  F1-Score: 0.7671\n",
      "  AUC-ROC: 0.8391\n",
      "  Improvement over Baseline: +-0.0007\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Experiment 2 - BatchNorm with SMOTE\n",
    "# Training on SMOTE resampled data\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 2: BATCHNORM + SMOTE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Loading SMOTE resampled data\n",
    "X_smote, y_smote = load_resampled_data(\n",
    "    method_name='smote',\n",
    "    data_dir=RESAMPLED_DIR\n",
    ")\n",
    "\n",
    "print(f\"\\nSMOTE dataset loaded: {len(X_smote):,} samples\")\n",
    "\n",
    "# Creating train-val split from SMOTE data\n",
    "X_train_smote, X_val_smote, y_train_smote, y_val_smote = train_test_split(\n",
    "    X_smote, y_smote,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED_VALUE,\n",
    "    stratify=y_smote\n",
    ")\n",
    "\n",
    "# Preparing data loaders\n",
    "train_loader_smote, val_loader_smote = prepare_data_loaders(\n",
    "    X_train=X_train_smote.values,\n",
    "    y_train=y_train_smote.values,\n",
    "    X_val=X_val_smote.values,\n",
    "    y_val=y_val_smote.values,\n",
    "    batch_size=DL_CONFIG['batch_count']\n",
    ")\n",
    "\n",
    "# Training model\n",
    "print(\"\\nTraining BatchNormClassifier on SMOTE data...\")\n",
    "model_smote = BatchNormClassifier(input_dim=input_dim)\n",
    "\n",
    "trained_smote, history_smote = train_pytorch_model(\n",
    "    model=model_smote,\n",
    "    train_loader=train_loader_smote,\n",
    "    val_loader=val_loader_smote,\n",
    "    epochs=DL_CONFIG['epoch_count'],\n",
    "    learning_rate=DL_CONFIG['learn_rate'],\n",
    "    pos_weight=None,\n",
    "    patience=DL_CONFIG['patience'],\n",
    "    device=DL_CONFIG['compute_device'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Evaluating\n",
    "smote_results = evaluate_pytorch_model(\n",
    "    model=trained_smote,\n",
    "    X_test=X_test.values,\n",
    "    y_test=y_test.values,\n",
    "    batch_size=DL_CONFIG['batch_count'],\n",
    "    device=DL_CONFIG['compute_device'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "smote_metrics = calculate_all_metrics(\n",
    "    y_true=y_test.values,\n",
    "    y_pred=smote_results['y_pred'],\n",
    "    y_proba=smote_results['y_proba']\n",
    ")\n",
    "\n",
    "print(\"\\nSMOTE PyTorch Performance:\")\n",
    "print(f\"  F1-Score: {smote_metrics['f1_score']:.4f}\")\n",
    "print(f\"  AUC-ROC: {smote_metrics['auc_roc']:.4f}\")\n",
    "print(f\"  Improvement over Baseline: +{smote_metrics['f1_score'] - baseline_metrics['f1_score']:.4f}\")\n",
    "\n",
    "# Storing results\n",
    "pytorch_results['smote_batchnorm'] = {\n",
    "    'model': trained_smote,\n",
    "    'history': history_smote,\n",
    "    'metrics': smote_metrics,\n",
    "    'predictions': smote_results\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a9952bc-f594-45dd-b9bb-2764d3952540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENT 3: BATCHNORM + BORDERLINE-SMOTE\n",
      "======================================================================\n",
      "Loading resampled data: borderline_smote\n",
      "  Loaded 847,476 samples\n",
      "  Features shape: (847476, 28)\n",
      "\n",
      "Borderline-SMOTE dataset loaded: 847,476 samples\n",
      "\n",
      "Training BatchNormClassifier on Borderline-SMOTE data...\n",
      "Epoch [5/50] - Train Loss: 0.5416, Train Acc: 0.7218, Val Loss: 0.5239, Val Acc: 0.7345\n",
      "Epoch [10/50] - Train Loss: 0.5297, Train Acc: 0.7304, Val Loss: 0.5137, Val Acc: 0.7414\n",
      "Epoch [15/50] - Train Loss: 0.5252, Train Acc: 0.7339, Val Loss: 0.5095, Val Acc: 0.7436\n",
      "Epoch [20/50] - Train Loss: 0.5223, Train Acc: 0.7353, Val Loss: 0.5070, Val Acc: 0.7459\n",
      "Epoch [25/50] - Train Loss: 0.5200, Train Acc: 0.7369, Val Loss: 0.5061, Val Acc: 0.7466\n",
      "Epoch [30/50] - Train Loss: 0.5181, Train Acc: 0.7380, Val Loss: 0.5037, Val Acc: 0.7487\n",
      "Epoch [35/50] - Train Loss: 0.5171, Train Acc: 0.7388, Val Loss: 0.5042, Val Acc: 0.7485\n",
      "Epoch [40/50] - Train Loss: 0.5160, Train Acc: 0.7395, Val Loss: 0.5027, Val Acc: 0.7501\n",
      "Epoch [45/50] - Train Loss: 0.5152, Train Acc: 0.7404, Val Loss: 0.5025, Val Acc: 0.7495\n",
      "Epoch [50/50] - Train Loss: 0.5143, Train Acc: 0.7405, Val Loss: 0.5016, Val Acc: 0.7507\n",
      "Training completed in 613.52 seconds\n",
      "Best validation loss: 0.5016\n",
      "Test Accuracy: 0.7518\n",
      "Prediction time: 1.6430 seconds\n",
      "\n",
      "Borderline-SMOTE PyTorch Performance:\n",
      "  F1-Score: 0.7562\n",
      "  Improvement over Baseline: +-0.0117\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Experiment 3 - BatchNorm with Borderline-SMOTE\n",
    "# Training on Borderline-SMOTE resampled data\n",
    "# Focus on boundary samples may improve classification\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 3: BATCHNORM + BORDERLINE-SMOTE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Loading Borderline-SMOTE data\n",
    "X_bsmote, y_bsmote = load_resampled_data(\n",
    "    method_name='borderline_smote',\n",
    "    data_dir=RESAMPLED_DIR\n",
    ")\n",
    "\n",
    "print(f\"\\nBorderline-SMOTE dataset loaded: {len(X_bsmote):,} samples\")\n",
    "\n",
    "# Creating train-val split\n",
    "X_train_bsmote, X_val_bsmote, y_train_bsmote, y_val_bsmote = train_test_split(\n",
    "    X_bsmote, y_bsmote,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED_VALUE,\n",
    "    stratify=y_bsmote\n",
    ")\n",
    "\n",
    "# Preparing data loaders\n",
    "train_loader_bsmote, val_loader_bsmote = prepare_data_loaders(\n",
    "    X_train=X_train_bsmote.values,\n",
    "    y_train=y_train_bsmote.values,\n",
    "    X_val=X_val_bsmote.values,\n",
    "    y_val=y_val_bsmote.values,\n",
    "    batch_size=DL_CONFIG['batch_count']\n",
    ")\n",
    "\n",
    "# Training\n",
    "print(\"\\nTraining BatchNormClassifier on Borderline-SMOTE data...\")\n",
    "model_bsmote = BatchNormClassifier(input_dim=input_dim)\n",
    "\n",
    "trained_bsmote, history_bsmote = train_pytorch_model(\n",
    "    model=model_bsmote,\n",
    "    train_loader=train_loader_bsmote,\n",
    "    val_loader=val_loader_bsmote,\n",
    "    epochs=DL_CONFIG['epoch_count'],\n",
    "    learning_rate=DL_CONFIG['learn_rate'],\n",
    "    pos_weight=None,\n",
    "    patience=DL_CONFIG['patience'],\n",
    "    device=DL_CONFIG['compute_device'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Evaluating\n",
    "bsmote_results = evaluate_pytorch_model(\n",
    "    model=trained_bsmote,\n",
    "    X_test=X_test.values,\n",
    "    y_test=y_test.values,\n",
    "    batch_size=DL_CONFIG['batch_count'],\n",
    "    device=DL_CONFIG['compute_device'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "bsmote_metrics = calculate_all_metrics(\n",
    "    y_true=y_test.values,\n",
    "    y_pred=bsmote_results['y_pred'],\n",
    "    y_proba=bsmote_results['y_proba']\n",
    ")\n",
    "\n",
    "print(\"\\nBorderline-SMOTE PyTorch Performance:\")\n",
    "print(f\"  F1-Score: {bsmote_metrics['f1_score']:.4f}\")\n",
    "print(f\"  Improvement over Baseline: +{bsmote_metrics['f1_score'] - baseline_metrics['f1_score']:.4f}\")\n",
    "\n",
    "# Storing results\n",
    "pytorch_results['borderline_smote_batchnorm'] = {\n",
    "    'model': trained_bsmote,\n",
    "    'history': history_bsmote,\n",
    "    'metrics': bsmote_metrics,\n",
    "    'predictions': bsmote_results\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d837adf-4644-411c-8a27-746882f9485c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENT 4: BATCHNORM + ADASYN\n",
      "======================================================================\n",
      "Loading resampled data: adasyn\n",
      "  Loaded 847,476 samples\n",
      "  Features shape: (847476, 28)\n",
      "\n",
      "ADASYN dataset loaded: 847,476 samples\n",
      "\n",
      "Training BatchNormClassifier on ADASYN data...\n",
      "Epoch [5/50] - Train Loss: 0.5321, Train Acc: 0.7292, Val Loss: 0.5151, Val Acc: 0.7420\n",
      "Epoch [10/50] - Train Loss: 0.5210, Train Acc: 0.7371, Val Loss: 0.5042, Val Acc: 0.7489\n",
      "Epoch [15/50] - Train Loss: 0.5158, Train Acc: 0.7408, Val Loss: 0.5006, Val Acc: 0.7510\n",
      "Epoch [20/50] - Train Loss: 0.5130, Train Acc: 0.7422, Val Loss: 0.4979, Val Acc: 0.7514\n",
      "Epoch [25/50] - Train Loss: 0.5109, Train Acc: 0.7442, Val Loss: 0.4962, Val Acc: 0.7531\n",
      "Epoch [30/50] - Train Loss: 0.5093, Train Acc: 0.7449, Val Loss: 0.4960, Val Acc: 0.7539\n",
      "Epoch [35/50] - Train Loss: 0.5075, Train Acc: 0.7459, Val Loss: 0.4948, Val Acc: 0.7554\n",
      "Epoch [40/50] - Train Loss: 0.5070, Train Acc: 0.7459, Val Loss: 0.4936, Val Acc: 0.7559\n",
      "Epoch [45/50] - Train Loss: 0.5061, Train Acc: 0.7472, Val Loss: 0.4927, Val Acc: 0.7560\n",
      "Epoch [50/50] - Train Loss: 0.5052, Train Acc: 0.7472, Val Loss: 0.4929, Val Acc: 0.7562\n",
      "Training completed in 595.50 seconds\n",
      "Best validation loss: 0.4921\n",
      "Test Accuracy: 0.7548\n",
      "Prediction time: 1.5546 seconds\n",
      "\n",
      "ADASYN PyTorch Performance:\n",
      "  F1-Score: 0.7671\n",
      "  Improvement over Baseline: +-0.0007\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Experiment 4 - BatchNorm with ADASYN\n",
    "# Purpose: Training on ADASYN resampled data\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 4: BATCHNORM + ADASYN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Loading ADASYN data\n",
    "X_adasyn, y_adasyn = load_resampled_data(\n",
    "    method_name='adasyn',\n",
    "    data_dir=RESAMPLED_DIR\n",
    ")\n",
    "\n",
    "print(f\"\\nADASYN dataset loaded: {len(X_adasyn):,} samples\")\n",
    "\n",
    "# Creating train-val split\n",
    "X_train_adasyn, X_val_adasyn, y_train_adasyn, y_val_adasyn = train_test_split(\n",
    "    X_adasyn, y_adasyn,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED_VALUE,\n",
    "    stratify=y_adasyn\n",
    ")\n",
    "\n",
    "# Preparing data loaders\n",
    "train_loader_adasyn, val_loader_adasyn = prepare_data_loaders(\n",
    "    X_train=X_train_adasyn.values,\n",
    "    y_train=y_train_adasyn.values,\n",
    "    X_val=X_val_adasyn.values,\n",
    "    y_val=y_val_adasyn.values,\n",
    "    batch_size=DL_CONFIG['batch_count']\n",
    ")\n",
    "\n",
    "# Training\n",
    "print(\"\\nTraining BatchNormClassifier on ADASYN data...\")\n",
    "model_adasyn = BatchNormClassifier(input_dim=input_dim)\n",
    "\n",
    "trained_adasyn, history_adasyn = train_pytorch_model(\n",
    "    model=model_adasyn,\n",
    "    train_loader=train_loader_adasyn,\n",
    "    val_loader=val_loader_adasyn,\n",
    "    epochs=DL_CONFIG['epoch_count'],\n",
    "    learning_rate=DL_CONFIG['learn_rate'],\n",
    "    pos_weight=None,\n",
    "    patience=DL_CONFIG['patience'],\n",
    "    device=DL_CONFIG['compute_device'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Evaluating\n",
    "adasyn_results = evaluate_pytorch_model(\n",
    "    model=trained_adasyn,\n",
    "    X_test=X_test.values,\n",
    "    y_test=y_test.values,\n",
    "    batch_size=DL_CONFIG['batch_count'],\n",
    "    device=DL_CONFIG['compute_device'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "adasyn_metrics = calculate_all_metrics(\n",
    "    y_true=y_test.values,\n",
    "    y_pred=adasyn_results['y_pred'],\n",
    "    y_proba=adasyn_results['y_proba']\n",
    ")\n",
    "\n",
    "print(\"\\nADASYN PyTorch Performance:\")\n",
    "print(f\"  F1-Score: {adasyn_metrics['f1_score']:.4f}\")\n",
    "print(f\"  Improvement over Baseline: +{adasyn_metrics['f1_score'] - baseline_metrics['f1_score']:.4f}\")\n",
    "\n",
    "# Storing results\n",
    "pytorch_results['adasyn_batchnorm'] = {\n",
    "    'model': trained_adasyn,\n",
    "    'history': history_adasyn,\n",
    "    'metrics': adasyn_metrics,\n",
    "    'predictions': adasyn_results\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08ccf804-f569-4bb1-ae48-1a2d2bcd4639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENT 5: WEIGHTED LOSS (Algorithm-Level Approach)\n",
      "======================================================================\n",
      "\n",
      "Approach: Using weighted BCE loss instead of data resampling\n",
      "Positive class weight: 0.8880\n",
      "\n",
      "Training WeightedLossClassifier with class weights...\n",
      "Epoch [5/50] - Train Loss: 0.5031, Train Acc: 0.7271, Val Loss: 0.4880, Val Acc: 0.7369\n",
      "Epoch [10/50] - Train Loss: 0.4924, Train Acc: 0.7348, Val Loss: 0.4797, Val Acc: 0.7422\n",
      "Epoch [15/50] - Train Loss: 0.4878, Train Acc: 0.7384, Val Loss: 0.4741, Val Acc: 0.7478\n",
      "Epoch [20/50] - Train Loss: 0.4851, Train Acc: 0.7398, Val Loss: 0.4726, Val Acc: 0.7467\n",
      "Epoch [25/50] - Train Loss: 0.4828, Train Acc: 0.7418, Val Loss: 0.4706, Val Acc: 0.7495\n",
      "Epoch [30/50] - Train Loss: 0.4817, Train Acc: 0.7418, Val Loss: 0.4700, Val Acc: 0.7482\n",
      "Epoch [35/50] - Train Loss: 0.4803, Train Acc: 0.7433, Val Loss: 0.4681, Val Acc: 0.7502\n",
      "Epoch [40/50] - Train Loss: 0.4793, Train Acc: 0.7445, Val Loss: 0.4682, Val Acc: 0.7508\n",
      "Epoch [45/50] - Train Loss: 0.4784, Train Acc: 0.7451, Val Loss: 0.4663, Val Acc: 0.7522\n",
      "Epoch [50/50] - Train Loss: 0.4776, Train Acc: 0.7454, Val Loss: 0.4659, Val Acc: 0.7516\n",
      "Training completed in 554.73 seconds\n",
      "Best validation loss: 0.4659\n",
      "Test Accuracy: 0.7538\n",
      "Prediction time: 1.4624 seconds\n",
      "\n",
      "Weighted Loss PyTorch Performance:\n",
      "  F1-Score: 0.7598\n",
      "  Improvement over Baseline: +-0.0081\n",
      "\n",
      "Comparison:\n",
      "  Baseline (No Treatment): 0.7679\n",
      "  Weighted Loss (Algorithm): 0.7598\n",
      "  SMOTE (Data-Level): 0.7671\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Experiment 5 - Weighted Loss (No Resampling)\n",
    "# Training with weighted BCE loss on original imbalanced data\n",
    "# Algorithm-level solution instead of data-level\n",
    "# Performance between baseline and resampled methods\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 5: WEIGHTED LOSS (Algorithm-Level Approach)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nApproach: Using weighted BCE loss instead of data resampling\")\n",
    "print(f\"Positive class weight: {pos_weight.item():.4f}\")\n",
    "\n",
    "# Using original baseline data loaders\n",
    "train_loader_weighted = train_loader_baseline\n",
    "val_loader_weighted = val_loader_baseline\n",
    "\n",
    "# Training with weighted loss\n",
    "print(\"\\nTraining WeightedLossClassifier with class weights...\")\n",
    "model_weighted_loss = WeightedLossClassifier(input_dim=input_dim)\n",
    "\n",
    "trained_weighted, history_weighted = train_pytorch_model(\n",
    "    model=model_weighted_loss,\n",
    "    train_loader=train_loader_weighted,\n",
    "    val_loader=val_loader_weighted,\n",
    "    epochs=DL_CONFIG['epoch_count'],\n",
    "    learning_rate=DL_CONFIG['learn_rate'],\n",
    "    pos_weight=pos_weight,  # Using class weights\n",
    "    patience=DL_CONFIG['patience'],\n",
    "    device=DL_CONFIG['compute_device'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Evaluating\n",
    "weighted_results = evaluate_pytorch_model(\n",
    "    model=trained_weighted,\n",
    "    X_test=X_test.values,\n",
    "    y_test=y_test.values,\n",
    "    batch_size=DL_CONFIG['batch_count'],\n",
    "    device=DL_CONFIG['compute_device'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "weighted_metrics = calculate_all_metrics(\n",
    "    y_true=y_test.values,\n",
    "    y_pred=weighted_results['y_pred'],\n",
    "    y_proba=weighted_results['y_proba']\n",
    ")\n",
    "\n",
    "print(\"\\nWeighted Loss PyTorch Performance:\")\n",
    "print(f\"  F1-Score: {weighted_metrics['f1_score']:.4f}\")\n",
    "print(f\"  Improvement over Baseline: +{weighted_metrics['f1_score'] - baseline_metrics['f1_score']:.4f}\")\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"  Baseline (No Treatment): {baseline_metrics['f1_score']:.4f}\")\n",
    "print(f\"  Weighted Loss (Algorithm): {weighted_metrics['f1_score']:.4f}\")\n",
    "print(f\"  SMOTE (Data-Level): {smote_metrics['f1_score']:.4f}\")\n",
    "\n",
    "# Storing results\n",
    "pytorch_results['weighted_loss'] = {\n",
    "    'model': trained_weighted,\n",
    "    'history': history_weighted,\n",
    "    'metrics': weighted_metrics,\n",
    "    'predictions': weighted_results\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d9b74-365a-4f10-98ec-76bfd1a54726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Higgs Project)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
